#engine
slots: 10
localAddress: localhost:8080
nodeZkAddress: node1:24002,node2:24002,node3:24002/flink154_xc
exeQueueSize: 1
#debug模式关闭数据校验
isDebug: true

isSecurity: true
userPrincipal: yanxi@HADOOP.COM
zkPrincipal: yanxi@HADOOP.COM
userKeytabPath: D:\kerberos\fusioninsight\yanxi_keytab\user.keytab
krb5ConfPath: D:\kerberos\fusioninsight\yanxi_keytab\krb5.conf

hadoopConfPath: /etc/hadoop/conf

db:
   url: jdbc:mysql://172.16.8.104:3306/rdos_test_spark1x?charset=utf8&autoReconnect=true
   userName: dtstack
   pwd: abc123

engineTypes:
    -
        typeName: flink150
        clusterMode: yarn
        flinkZkAddress: node1:24002,node2:24002,node3:24002
        #简化为只配置/flink150/ha
        flinkHighAvailabilityStorageDir: /flink150/ha
        flinkZkNamespace: /flink150
        jarTmpDir: ../tmp150
        flinkPluginRoot: D:\flinkplugin
        remotePluginRootDir: /opt/dtstack/150_flinkplugin
        #新增配置
        flinkYarnMode: PER_JOB # flinkOnYarn 三种模式： new、legacy、per_job
        #new
        #flinkYarnNewModeMaxSlots: 4
        #perjob
        #flinkConfigDir: /opt/dtstack/flink-1.5.0/conf
        flinkJarPath: /Users/jiangjunjie/Documents/OpenCode/flink/flink-dist/target/flink-1.5.4-bin/flink-1.5.4/lib
        #flinkJobHistory: http://kudu1:8082


    #-
    #    typeName: flink140
    #    clusterMode: standalone
    #    flinkJobMgrUrl: localhost:6123
    #    jarTmpDir: ../tmp140
    #    flinkPluginRoot: E:\flinkplugin
    #    remotePluginRootDir: E:\flinkplugin

    #-
    #    typeName: spark_yarn
    #    sparkYarnArchive: hdfs://kudu1:9000/sparkjars/jars
    #    sparkSqlProxyPath: hdfs://kudu1:9000/user/spark/spark-0.0.1-SNAPSHOT.jar
    #    sparkSqlProxyMainClass: com.dtstack.sql.main.SqlProxy
    #    sparkPythonExtLibPath: hdfs://kudu1:9000/pythons/pyspark.zip,hdfs://kudu1:9000/pythons/py4j-0.10.4-src.zip

    #-
    #    typeName: datax
    #    dataxSSHAddress: 172.16.1.155 #可以多个
    #    userName: root
    #    password: abc123 #可以不填 打通ssh免登陆
    #    dataxBinDir: /opt/dtstack/datax/bin

    #-
    #    typeName: spark
    #    sparkWebMaster: node03:8080,node02:8080
    #    sparkMaster: spark://node03:6066,node02:6066
    #    sparkSqlProxyPath: hdfs://ns1/user/spark/spark-0.0.1-SNAPSHOT.jar
    #    sparkSqlProxyMainClass: com.dtstack.sql.main.SqlProxy

    #-
    #    typeName: mysql
    #    dbUrl: jdbc:mysql://172.16.8.104:3306/rdos?charset=utf8
    #    userName: dtstack
    #    pwd: abc123

#    -
#        typeName: spark_yarn
#        sparkYarnArchive: hdfs://kudu1:9000/sparkjars/jars
#        sparkSqlProxyPath: hdfs://kudu1:9000/hyf/spark-0.0.1-SNAPSHOT.jar
#        sparkSqlProxyMainClass: com.dtstack.sql.main.SqlProxy
#        sparkPythonExtLibPath: hdfs://kudu1:9000/pythons/pyspark.zip,hdfs://kudu1:9000/pythons/py4j-0.10.4-src.zip
#        hadoopConf:
#            fs.defaultFS: hdfs://ns1
#            dfs.ha.namenodes.ns1: nn1,nn2
#            dfs.namenode.rpc-address.ns1.nn2: node03:9000
#            dfs.client.failover.proxy.provider.ns1: org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
#            dfs.namenode.rpc-address.ns1.nn1: node02:9000
#            dfs.nameservices: ns1
#            fs.hdfs.impl.disable.cache: true
#            fs.hdfs.impl: org.apache.hadoop.hdfs.DistributedFileSystem
#        yarnConf:
#            yarn.resourcemanager.address.rm1: node02:8032
#            yarn.resourcemanager.webapp.address.rm2: node03:8088
#            yarn.resourcemanager.webapp.address.rm1: node02:8088
#            yarn.resourcemanager.ha.rm-ids: rm1,rm2
#            yarn.resourcemanager.address.rm2: node03:8032
#            yarn.resourcemanager.ha.enabled: true
